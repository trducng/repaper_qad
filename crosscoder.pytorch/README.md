- [x] Rerun dead neurons ref to check whether the percentage of dead neurons stay the same
    - @RESULT: Confirm that the percentage of dead neurons stay the same (2284 neurons when decoder weights is inited at 0.08)
- [x] Check the effect of decoder init norm on the number of dead neurons for the reference model
    - @RESULT:
        - 0.03: 4669
        - 0.1: 2088
        - 0.12: 1944
        - 0.14: 2164
        - 0.16: 2480
        - 0.18: 3398
        - 1: 1.18e+4
        - There exists a sweet spot for the decoder init norm, such that the number of dead neurons can decrease.
        - The decoder weight also plays an important role in calculating the regularization term.
- [x] Check dead neurons of 1A
    - @Result -> Quite good.
        - No dead neurons
        - Why is that? This is so weird.
- [x] Use correct transpose version for kaiming init
    - @Result -> Dead neurons become 5128
- [x] Normalize the weight after kaiming init
    - @Result:
        - 0.12: Dead neurons become 65. It is an acceptable number. 0.0228
          percent of features are activated on average.
- [ ] The reconstruction is shit. It's even worse than just switching layer.
    - On median, an input has value of 1.2796. On median, we mis-predict by 1.2575. So, we are off by nearly 100%.
    - [ ] Don't involve the decoder in the reg section
- [ ] Incorprate the explained variance into metrics
- [ ] Check if loading dataset is efficient
    - It's currently not. It has error in that even doesn't iterate through the whole dataset.
    - You want to achieve:
        - Multiple processeses sharing the same block of data on RAM.
        - Preload the data from disk to RAM during computational processes, so that we decrease I/O bottleneck from computation time.
        - Preload the data from RAM to GPU memory during computational processes, so that we decrease I/O bottleneck from computation time.
        - Add more SSD to store read-intensive data.
        - Add more RAM to reduce the frequency of copying data.

- [ ] Experiment management system, where you can submit your job queue
    - [ ] Can get some hands on experience with Slurm
    - Experiment management is quite tricky:
        - Correct data when the job is submitted.
        - Correct code when the job is submitted.
        - Correct configuration when the job is submitted.
        - Have the correct environment to run the training.
        - Multi-job management (when running jobs in parallel).
    - The job progress will be shown in the service (in the terminal) and in
      tensorboard...
    - You can have another comment to push the job to a queue.
    - And then the services will execute that job.
    - You can list remaining jobs, modifying the remaining jobs, or deleting the jobs.
    - You can list hardware accelerators on multiple machines.
    - You can push job so that it will be executed in a different machines.
    - Sounds like a more flexible vast.ai or prime intellect, or petals.
